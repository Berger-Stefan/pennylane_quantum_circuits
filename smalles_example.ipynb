{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "#import jaxopt\n",
    "import optax\n",
    "import catalyst\n",
    "\n",
    "n_wires = 2\n",
    "\n",
    "@qml.qnode(qml.device(\"lightning.qubit\", wires=n_wires), diff_method=\"best\")\n",
    "def circuit(x, theta):\n",
    "\n",
    "    for i in range(n_wires):\n",
    "        qml.RY(2*jnp.arccos(x),wires = i)\n",
    "    \n",
    "    for i in range(n_wires):\n",
    "        qml.RX(theta[i, 0], wires=i)\n",
    "        qml.RY(theta[i, 1], wires=i)\n",
    "        qml.RX(theta[i, 2], wires=i)\n",
    "        qml.CNOT(wires=[i, (i + 1) % n_wires])\n",
    "\n",
    "    return qml.expval(qml.sum(*[qml.PauliZ(i) for i in range(n_wires)]))\n",
    "\n",
    "def loss_fnc(theta):\n",
    "    x = jnp.linspace(0,.9,11)\n",
    "    _dudx = jax.grad(circuit, argnums=0)\n",
    "    dudx = jnp.array([_dudx(i, theta) for i in x])\n",
    "    \n",
    "    u_0 = circuit(x[0], theta)\n",
    "    \n",
    "    loss_1 = jnp.mean(u_0**2)\n",
    "    loss_2 = jnp.mean((dudx - jnp.ones_like(dudx))**2)\n",
    "\n",
    "    return loss_1 + loss_2\n",
    "\n",
    "theta = jnp.ones((n_wires,3))\n",
    "opt = optax.adam(learning_rate=0.01)\n",
    "opt_state = opt.init(theta)\n",
    "\n",
    "def optimize(theta, opt_state):\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(100):\n",
    "        loss_val, grads = jax.value_and_grad(loss_fnc)(theta)\n",
    "        updates, opt_state = opt.update(grads, opt_state)\n",
    "        theta = optax.apply_updates(theta, updates)\n",
    "        print(f\"Step: {i}  Loss: {loss_val}\")\n",
    "        loss_history.append(loss_val)\n",
    "\n",
    "    return theta, opt_state, loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0  Loss: 6.206298278863095\n",
      "Step: 1  Loss: 5.973365632428284\n",
      "Step: 2  Loss: 5.74568701590633\n",
      "Step: 3  Loss: 5.523665986970554\n",
      "Step: 4  Loss: 5.307676035139273\n",
      "Step: 5  Loss: 5.098056012364894\n",
      "Step: 6  Loss: 4.895106012314087\n",
      "Step: 7  Loss: 4.699083860209125\n",
      "Step: 8  Loss: 4.510202362141512\n",
      "Step: 9  Loss: 4.328627438612831\n"
     ]
    }
   ],
   "source": [
    "theta, opt_state, loss_history = optimize(theta, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(12)\n",
    "\n",
    "# Add energy plot on column 1\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(range(len(loss_history)), loss_history, \"go\", ls=\"dashed\")\n",
    "ax1.set_xlabel(\"Optimization step\", fontsize=13)\n",
    "ax1.set_ylabel(\"Loss\", fontsize=13)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "x = jnp.linspace(0,1,21)\n",
    "f_qc = my_model(x, params[\"weights\"], params[\"bias\"])\n",
    "f_an = target_fnc(x)\n",
    "ax2.plot( x, f_qc, \"ro\", ls=\"dashed\")\n",
    "ax2.plot( x, f_an, \"go\", ls=\"dashed\")\n",
    "ax2.legend([\"QCML\", \"Analytical\"])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
