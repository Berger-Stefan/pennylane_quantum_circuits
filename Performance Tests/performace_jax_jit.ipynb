{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  Loss: 0.28363654017448425\n",
      "Step: 200  Loss: 0.07567053288221359\n",
      "Step: 300  Loss: 0.09308571368455887\n",
      "Step: 400  Loss: 0.04231073707342148\n",
      "Step: 500  Loss: 0.015424626879394054\n",
      "Step: 600  Loss: 0.012171284295618534\n",
      "Step: 700  Loss: 0.011967191472649574\n",
      "Step: 800  Loss: 0.03056567907333374\n",
      "Step: 900  Loss: 0.010834799148142338\n",
      "Step: 1000  Loss: 0.01202180702239275\n",
      "2min 59s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "\n",
    "n_wires = 4\n",
    "weights = jnp.ones((n_wires,3))\n",
    "bias = jnp.array(0.)\n",
    "opt = optax.adam(learning_rate=0.1)\n",
    "params = {\"weights\": weights, \"bias\": bias}\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "@qml.qnode(qml.device(\"default.qubit\", wires=n_wires), diff_method=\"best\")\n",
    "def circuit(x, weights):\n",
    "\n",
    "    # Embedding Ansatz\n",
    "    for i in range(n_wires):\n",
    "        qml.RY(2*jnp.arccos(x),wires = i)\n",
    "\n",
    "    # Variational Ansatz   \n",
    "    for i in range(n_wires):\n",
    "        qml.RX(weights[i, 0], wires=i)\n",
    "        qml.RY(weights[i, 1], wires=i)\n",
    "        qml.RX(weights[i, 2], wires=i)\n",
    "        qml.CNOT(wires=[i, (i + 1) % n_wires])\n",
    "\n",
    "    # Total magnetization in z-direction as cost function\n",
    "    return qml.expval(qml.sum(*[qml.PauliZ(i) for i in range(n_wires)]))\n",
    "\n",
    "@jax.jit\n",
    "def my_model(data, weights, bias):\n",
    "    return circuit(data, weights) + bias\n",
    "\n",
    "@jax.jit\n",
    "def loss_fnc(params):\n",
    "    # Loss function of: du/dx = 1, u(0) = 0\n",
    "    x = jnp.linspace(0,0.99,11)\n",
    "    _dudx = jax.grad(my_model, argnums=0)\n",
    "    dudx = jnp.array([_dudx(i, params[\"weights\"], params[\"bias\"]) for i in x])\n",
    "    \n",
    "    loss_diff = jnp.mean((dudx - jnp.ones_like(dudx))**2)\n",
    "    loss_initial = jnp.mean(my_model(jnp.zeros_like(x),params[\"weights\"], params[\"bias\"])**2)\n",
    "    \n",
    "    return loss_diff + loss_initial\n",
    "\n",
    "def optimize(params, opt_state, n=1000):\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        loss_val, grads = jax.value_and_grad(loss_fnc)(params)\n",
    "        updates, opt_state = opt.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        if i%100 == 0: jax.debug.print(\"Step: {i}  Loss: {loss_val}\", i=i, loss_val=loss_val)\n",
    "        loss_history.append(loss_val)\n",
    "\n",
    "    return params, opt_state, loss_history\n",
    "\n",
    "%timeit -r1 -n1 optimize(params, opt_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  Loss: 0.28363654017448425\n",
      "Step: 200  Loss: 0.07567053288221359\n",
      "Step: 300  Loss: 0.09308571368455887\n",
      "Step: 400  Loss: 0.04231073707342148\n",
      "Step: 500  Loss: 0.015424626879394054\n",
      "Step: 600  Loss: 0.012171284295618534\n",
      "Step: 700  Loss: 0.011967191472649574\n",
      "Step: 800  Loss: 0.03056567907333374\n",
      "Step: 900  Loss: 0.010834799148142338\n",
      "Step: 1000  Loss: 0.01202180702239275\n",
      "51.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 optimize(params, opt_state)\n",
    "# params, opt_state, loss_history = optimize(params, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(12)\n",
    "\n",
    "# Add energy plot on column 1\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(range(len(loss_history)), loss_history, \"go\", ls=\"dashed\")\n",
    "ax1.set_xlabel(\"Optimization step\", fontsize=13)\n",
    "ax1.set_ylabel(\"Loss\", fontsize=13)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "x = jnp.linspace(0,0.99,21)\n",
    "f_qc = my_model(x,params[\"weights\"], params[\"bias\"])\n",
    "f_an = x\n",
    "ax2.plot( x, f_qc, \"ro\", ls=\"dashed\")\n",
    "ax2.plot( x, f_an, \"go\", ls=\"dashed\")\n",
    "ax2.legend([\"QCML\", \"Analytical\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
