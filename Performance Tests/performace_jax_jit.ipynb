{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The objective function must be encoded as a single QNode for the natural gradient to be automatically computed. Otherwise, metric_tensor_fn must be explicitly provided to the optimizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 61\u001b[0m\n\u001b[1;32m     57\u001b[0m         loss_history\u001b[38;5;241m.\u001b[39mappend(loss_val)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m params, loss_history\n\u001b[0;32m---> 61\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-r1 -n1 optimize(params)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/IPython/core/magics/execution.py:1189\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[1;32m   1187\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m all_runs \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n\u001b[1;32m   1191\u001b[0m worst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(all_runs) \u001b[38;5;241m/\u001b[39m number\n",
      "File \u001b[0;32m/usr/lib/python3.11/timeit.py:208\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    206\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[0;32m--> 208\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/IPython/core/magics/execution.py:173\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    171\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(params, n)\u001b[0m\n\u001b[1;32m     48\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# loss_val, grads = jax.value_and_grad(loss_fnc)(params)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# updates, opt_state = opt.update(grads, opt_state)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# params = optax.apply_updates(params, updates)\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fnc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     loss_val \u001b[38;5;241m=\u001b[39m loss_fnc(params)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: jax\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep: \u001b[39m\u001b[38;5;132;01m{i}\u001b[39;00m\u001b[38;5;124m  Loss: \u001b[39m\u001b[38;5;132;01m{loss_val}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, i\u001b[38;5;241m=\u001b[39mi, loss_val\u001b[38;5;241m=\u001b[39mloss_val)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pennylane/optimize/qng.py:253\u001b[0m, in \u001b[0;36mQNGOptimizer.step\u001b[0;34m(self, qnode, grad_fn, recompute_tensor, metric_tensor_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m, qnode, \u001b[38;5;241m*\u001b[39margs, grad_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, recompute_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, metric_tensor_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    231\u001b[0m ):\n\u001b[1;32m    232\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update the parameter array :math:`x` with one step of the optimizer.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m        array: the new variable values :math:`x^{(t+1)}`\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     new_args, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_and_cost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqnode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecompute_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecompute_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_tensor_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_tensor_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_args\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pennylane/optimize/qng.py:182\u001b[0m, in \u001b[0;36mQNGOptimizer.step_and_cost\u001b[0;34m(self, qnode, grad_fn, recompute_tensor, metric_tensor_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# pylint: disable=arguments-differ\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(qnode, qml\u001b[38;5;241m.\u001b[39mQNode) \u001b[38;5;129;01mand\u001b[39;00m metric_tensor_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe objective function must be encoded as a single QNode for the natural gradient \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be automatically computed. Otherwise, metric_tensor_fn must be explicitly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovided to the optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m     )\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recompute_tensor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metric_tensor_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: The objective function must be encoded as a single QNode for the natural gradient to be automatically computed. Otherwise, metric_tensor_fn must be explicitly provided to the optimizer."
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "\n",
    "n_wires = 4\n",
    "weights = jnp.ones((n_wires,3))\n",
    "bias = jnp.array(0.)\n",
    "params = {\"weights\": weights, \"bias\": bias}\n",
    "#opt = optax.adam(learning_rate=0.1)\n",
    "#opt_state = opt.init(params)\n",
    "opt = qml.QNGOptimizer()\n",
    "\n",
    "\n",
    "@qml.qnode(qml.device(\"default.qubit\", wires=n_wires), diff_method=\"best\")\n",
    "def circuit(x, weights):\n",
    "\n",
    "    # Embedding Ansatz\n",
    "    for i in range(n_wires):\n",
    "        qml.RY(2*jnp.arccos(x),wires = i)\n",
    "\n",
    "    # Variational Ansatz   \n",
    "    for i in range(n_wires):\n",
    "        qml.RX(weights[i, 0], wires=i)\n",
    "        qml.RY(weights[i, 1], wires=i)\n",
    "        qml.RX(weights[i, 2], wires=i)\n",
    "        qml.CNOT(wires=[i, (i + 1) % n_wires])\n",
    "\n",
    "    # Total magnetization in z-direction as cost function\n",
    "    return qml.expval(qml.sum(*[qml.PauliZ(i) for i in range(n_wires)]))\n",
    "\n",
    "@jax.jit\n",
    "def my_model(data, weights, bias):\n",
    "    return circuit(data, weights) + bias\n",
    "\n",
    "@jax.jit\n",
    "def loss_fnc(params):\n",
    "    # Loss function of: du/dx = 1, u(0) = 0\n",
    "    x = jnp.linspace(0,0.99,21)\n",
    "    _dudx = jax.grad(my_model, argnums=0)\n",
    "    dudx = jnp.array([_dudx(i, params[\"weights\"], params[\"bias\"]) for i in x])\n",
    "    \n",
    "    res = dudx - (4 * x**3 + x**2 - 2 * x - 0.5)\n",
    "    loss_diff = jnp.mean(res**2)\n",
    "    loss_initial = jnp.mean((my_model(jnp.zeros_like(x),params[\"weights\"], params[\"bias\"]) - jnp.ones_like(x))**2 )\n",
    "    \n",
    "def optimize(params, n=1000):\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        # loss_val, grads = jax.value_and_grad(loss_fnc)(params)\n",
    "        # updates, opt_state = opt.update(grads, opt_state)\n",
    "        # params = optax.apply_updates(params, updates)\n",
    "        params = opt.step(loss_fnc, params)\n",
    "        loss_val = loss_fnc(params)\n",
    "        if i%100 == 0: jax.debug.print(\"Step: {i}  Loss: {loss_val}\", i=i, loss_val=loss_val)\n",
    "        loss_history.append(loss_val)\n",
    "\n",
    "    return params, loss_history\n",
    "\n",
    "%timeit -r1 -n1 optimize(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  Loss: 0.28363654017448425\n",
      "Step: 200  Loss: 0.07567053288221359\n",
      "Step: 300  Loss: 0.09308571368455887\n",
      "Step: 400  Loss: 0.04231073707342148\n",
      "Step: 500  Loss: 0.015424626879394054\n",
      "Step: 600  Loss: 0.012171284295618534\n",
      "Step: 700  Loss: 0.011967191472649574\n",
      "Step: 800  Loss: 0.03056567907333374\n",
      "Step: 900  Loss: 0.010834799148142338\n",
      "Step: 1000  Loss: 0.01202180702239275\n",
      "51.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 optimize(params, opt_state)\n",
    "# params, opt_state, loss_history = optimize(params, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(12)\n",
    "\n",
    "# Add energy plot on column 1\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(range(len(loss_history)), loss_history, \"go\", ls=\"dashed\")\n",
    "ax1.set_xlabel(\"Optimization step\", fontsize=13)\n",
    "ax1.set_ylabel(\"Loss\", fontsize=13)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "x = jnp.linspace(0,0.99,21)\n",
    "f_qc = my_model(x,params[\"weights\"], params[\"bias\"])\n",
    "f_an = x\n",
    "ax2.plot( x, f_qc, \"ro\", ls=\"dashed\")\n",
    "ax2.plot( x, f_an, \"go\", ls=\"dashed\")\n",
    "ax2.legend([\"QCML\", \"Analytical\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
