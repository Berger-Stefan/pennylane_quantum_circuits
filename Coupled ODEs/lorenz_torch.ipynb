{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from IPython import display\n",
    "\n",
    "## QCircuit parameter\n",
    "n_wires = 4\n",
    "n_layers = 3\n",
    "\n",
    "## Optimizer Settings\n",
    "boundary_loss_scaling = 100\n",
    "\n",
    "## ODE parameter\n",
    "rho = 13\n",
    "sigma = 10\n",
    "beta = 8/3\n",
    "# Solver settings\n",
    "t_start = 0.0001\n",
    "t_end   = 5.0\n",
    "n_steps = 100000\n",
    "# Boundary values\n",
    "x_0 = 2\n",
    "y_0 = 1\n",
    "z_0 = 1\n",
    "\n",
    "\n",
    "def derivatives_fnc(t, x):\n",
    "    dx_dt = sigma*(x[1] - x[0])\n",
    "    dy_dt = x[0]*(rho-x[2]) - x[1]\n",
    "    dz_dt = x[0]*x[1] - beta*x[2]\n",
    "    # dx_dt = -1\n",
    "    # dy_dt = -1\n",
    "    # dz_dt = -1\n",
    "    return (dx_dt, dy_dt, dz_dt)\n",
    "\n",
    "t = torch.linspace(t_start,t_end,n_steps)  \n",
    "analytical_solution = torch.tensor(solve_ivp(derivatives_fnc, [t_start,t_end+0.000001], [x_0,y_0,z_0], t_eval=t).y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@qml.qnode(qml.device(\"default.qubit\", wires=n_wires), diff_method=\"best\")\n",
    "def circuit(x, weights):\n",
    "    # Embedding\n",
    "    for i in range(n_wires):\n",
    "        qml.RY(2*i*torch.arccos((x-t_end/2)/(t_end)),wires = i)\n",
    "    # Variational ansatz\n",
    "    qml.BasicEntanglerLayers(weights=weights, wires=range(n_wires))\n",
    "    # Cost function\n",
    "    return qml.expval(qml.sum(*[qml.PauliZ(i) for i in range(n_wires)]))\n",
    "\n",
    "def my_model(x:torch.Tensor, weights:list[torch.Tensor], biases:list[torch.Tensor], scaling:list[torch.Tensor]) ->torch.Tensor:\n",
    "    vcircuit = torch.vmap(circuit, in_dims=(0,None))\n",
    "    xyz = torch.vstack([scaling[0] * vcircuit(x, weights[0]) + biases[0],\n",
    "            scaling[1] * vcircuit(x, weights[1]) + biases[1],\n",
    "            scaling[2] * vcircuit(x, weights[2]) + biases[2]])\n",
    "    return xyz\n",
    "\n",
    "def loss_diff_fnc(t:torch.Tensor, weights:list[torch.Tensor], biases:list[torch.Tensor], scaling:list[torch.Tensor]) ->torch.Tensor:\n",
    "\n",
    "    [x_pred, y_pred, z_pred] = my_model(t, weights, biases, scaling)\n",
    "\n",
    "    grad_outputs_1 = torch.ones_like(x_pred)\n",
    "    dx_dt_pred = autograd.grad(x_pred, t, grad_outputs=grad_outputs_1, create_graph=True)[0]\n",
    "    grad_outputs_2 = torch.ones_like(y_pred)\n",
    "    dy_dt_pred = autograd.grad(y_pred, t, grad_outputs=grad_outputs_2, create_graph=True)[0]\n",
    "    grad_outputs_3 = torch.ones_like(z_pred)\n",
    "    dz_dt_pred = autograd.grad(z_pred, t, grad_outputs=grad_outputs_3, create_graph=True)[0]\n",
    "\n",
    "    # ODE loss\n",
    "    dx_dt, dy_dt, dz_dt = derivatives_fnc(t, (x_pred, y_pred, z_pred))\n",
    "    res_1 = dx_dt_pred - dx_dt\n",
    "    res_2 = dy_dt_pred - dy_dt\n",
    "    res_3 = dz_dt_pred - dz_dt\n",
    "    loss_pde = torch.mean(res_1**2) + torch.mean(res_2**2) + torch.mean(res_3**2)    \n",
    "    return loss_pde\n",
    "\n",
    "def loss_boundary_fnc(t:torch.Tensor, weights:list[torch.Tensor], biases:list[torch.Tensor], scaling:list[torch.Tensor]) ->torch.Tensor:\n",
    "    [x_0_pred, y_0_pred, z_0_pred] = my_model(torch.zeros_like(t), weights, biases, scaling)\n",
    "    loss_boundary = torch.mean((x_0_pred - x_0)**2) + torch.mean((y_0_pred - y_0)**2) + torch.mean((z_0_pred - z_0)**2)\n",
    "    return boundary_loss_scaling * loss_boundary\n",
    "\n",
    "def loss_fnc(t:torch.Tensor, weights:list[torch.Tensor], biases:list[torch.Tensor], scaling:list[torch.Tensor]) ->torch.Tensor:\n",
    "\n",
    "    loss_diff     = loss_diff_fnc(t, weights, biases, scaling)\n",
    "    loss_boundary = loss_boundary_fnc(t, weights, biases, scaling)\n",
    "\n",
    "    return loss_boundary + loss_diff\n",
    "\n",
    "def loss_analytical_fnc(weights, biases, scaling):\n",
    "    t = torch.linspace(t_start,t_end,n_steps, requires_grad=True)\n",
    "    [x_pred, y_pred, z_pred] = my_model(t, weights, biases, scaling)\n",
    "\n",
    "    return torch.mean((x_pred - analytical_solution[0])**2) + torch.mean((y_pred - analytical_solution[1])**2) + torch.mean((z_pred - analytical_solution[2])**2)\n",
    "    \n",
    "\n",
    "def optimize(n_iter = 100, verbose=0):\n",
    "    # Solve ODE numerically\n",
    "    t = torch.linspace(t_start,t_end,n_steps, requires_grad=True)\n",
    "\n",
    "    def update(verbose):\n",
    "        if verbose == 1:\n",
    "            print(f\"Step: {i}  Loss: {loss}\")\n",
    "        elif verbose == 2:\n",
    "            pass\n",
    "            fig = plt.figure()\n",
    "            fig.set_figheight(5)\n",
    "            fig.set_figwidth(20)\n",
    "\n",
    "            # Add energy plot on column 1\n",
    "            ax1 = fig.add_subplot(131)\n",
    "            ax1.grid()\n",
    "            ax1.plot(range(len(loss_history)), loss_history, \"black\", ls=\"solid\")\n",
    "            ax1.plot(range(len(loss_diff_history)), loss_diff_history, \"g\", ls=\"dotted\")\n",
    "            ax1.plot(range(len(loss_boundary_history)), loss_boundary_history, \"r\", ls=\"dashed\")\n",
    "            ax1.plot(range(len(loss_analytical_history)), loss_analytical_history, \"y\", ls=\"dashdot\")\n",
    "            ax1.legend([\"total loss\", \"diff loss\", \"boundary loss\", \"analytical loss\"], fontsize=9, loc=1)\n",
    "            ax1.set_yscale('log')\n",
    "            ax1.set_xlabel(\"Optimization step\", fontsize=13)\n",
    "            ax1.set_ylabel(\"Loss\", fontsize=13)\n",
    "            ax1.set_title(\"Loss\",fontsize=16)\n",
    "\n",
    "            ax2 = fig.add_subplot(132, projection='3d')\n",
    "            ax2.plot(analytical_solution[0], analytical_solution[1], analytical_solution[2], \"r\",lw=1)\n",
    "            t = torch.linspace(t_start, t_end, n_steps, requires_grad=True)\n",
    "            [x,y,z] = my_model(t, weights, biases, scaling)\n",
    "            ax2.plot(x.detach(),y.detach(),z.detach(), \"g\",lw=1)\n",
    "            ax2.legend([\"analytical\", \"quantum\"], fontsize=13)\n",
    "            ax2.set_xlabel(\"x\", fontsize=13)\n",
    "            ax2.set_ylabel(\"y\", fontsize=13)\n",
    "            ax2.set_zlabel(\"z\", fontsize=13)\n",
    "            ax2.set_title(\"Trajectory\",fontsize=16)\n",
    "            \n",
    "            ax3 = fig.add_subplot(133)\n",
    "            ax3.plot(t.detach(), analytical_solution[0], \"r\", ls=\"solid\",linewidth=5, alpha=0.3)\n",
    "            ax3.plot(t.detach(), x.detach(), \"r\", ls=\"dashed\",linewidth=2)\n",
    "            ax3.plot(t.detach(), analytical_solution[1], \"b\", ls=\"solid\",linewidth=5, alpha=0.3)\n",
    "            ax3.plot(t.detach(), y.detach(), \"b\", ls=\"dashed\",linewidth=2)\n",
    "            ax3.plot(t.detach(), analytical_solution[2], \"g\", ls=\"solid\",linewidth=5, alpha=0.3)\n",
    "            ax3.plot(t.detach(), z.detach(), \"g\", ls=\"dashed\",linewidth=2)\n",
    "            ax3.legend([\"x_exact\",\"x_quantum\",\"y_exact\",\"y_quantum\",\"z_exact\",\"z_quantum\"], fontsize=11)\n",
    "            ax3.set_xlabel(\"t\", fontsize=13)\n",
    "            ax3.set_ylabel(\"[x,y,z]\", fontsize=13)\n",
    "            ax3.set_title(\"Evolution\",fontsize=16)\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "            plt.show()\n",
    "            print(f\"Step: {i}  Loss: {loss}\")\n",
    "    \n",
    "    for i in range(1,n_iter+1):\n",
    "        opt.zero_grad()\n",
    "        loss = loss_fnc(t,weights, biases, scaling)\n",
    "        #loss = loss_analytical_fnc(weights, biases, scaling)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        loss_history.append(loss.detach())\n",
    "        loss_diff_history.append(loss_diff_fnc(t, weights, biases, scaling).detach())\n",
    "        loss_boundary_history.append(loss_boundary_fnc(t, weights, biases, scaling).detach())\n",
    "        loss_analytical_history.append(loss_analytical_fnc(weights, biases, scaling).detach())\n",
    "\n",
    "        if i%10 == 0 : update(verbose)\n",
    "\n",
    "weights = [torch.rand((n_layers, n_wires), requires_grad=True), \n",
    "           torch.rand((n_layers, n_wires), requires_grad=True),\n",
    "           torch.rand((n_layers, n_wires), requires_grad=True)]\n",
    "biases = [torch.rand(1, requires_grad=True), \n",
    "          torch.rand(1, requires_grad=True),\n",
    "          torch.rand(1, requires_grad=True)]\n",
    "scaling = [torch.rand(1, requires_grad=True), \n",
    "          torch.rand(1, requires_grad=True),\n",
    "          torch.rand(1, requires_grad=True)]\n",
    "parameters = weights + biases + scaling\n",
    "\n",
    "# Create optimizer\n",
    "opt = torch.optim.Adam(parameters, lr=0.01)\n",
    "loss_history = []\n",
    "loss_diff_history = []\n",
    "loss_boundary_history = []\n",
    "loss_analytical_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit -n1 -r1 \n",
    "opt.param_groups[0][\"lr\"] = 0.1\n",
    "%timeit -n1 -r1  optimize(n_iter=300, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Following Code is only a sanity check for the derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(t_start,t_end,n_steps, requires_grad=True)\n",
    "[x_pred, y_pred, z_pred] = my_model(t, weights, biases, scaling)\n",
    "\n",
    "grad_outputs_1 = torch.ones_like(x_pred)\n",
    "dx_dt_pred = autograd.grad(x_pred, t, grad_outputs=grad_outputs_1, create_graph=True)[0]\n",
    "grad_outputs_2 = torch.ones_like(y_pred)\n",
    "dy_dt_pred = autograd.grad(y_pred, t, grad_outputs=grad_outputs_2, create_graph=True)[0]\n",
    "grad_outputs_3 = torch.ones_like(z_pred)\n",
    "dz_dt_pred = autograd.grad(z_pred, t, grad_outputs=grad_outputs_3, create_graph=True)[0]\n",
    "\n",
    "# ODE loss\n",
    "dx_dt, dy_dt, dz_dt = derivatives_fnc(t, (x_pred, y_pred, z_pred))\n",
    "res_1 = dx_dt_pred - dx_dt\n",
    "res_2 = dy_dt_pred - dy_dt\n",
    "res_3 = dz_dt_pred - dz_dt\n",
    "loss_pde = torch.mean(res_1**2) + torch.mean(res_2**2) + torch.mean(res_3**2)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0017, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(res_3**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = t[1]-t[0]\n",
    "fd_x_pred = torch.diff(x_pred)/delta_t \n",
    "fd_x = torch.diff(analytical_solution[0])/delta_t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0000, -1.0249]),)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.linspace(t_start,t_end,n_steps, requires_grad=True)\n",
    "[x_pred, y_pred, z_pred] = my_model(t, weights, biases, scaling)\n",
    "torch.autograd.grad(scaling[0] * circuit(t[1], weights[0]), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(t_start,t_end,n_steps)\n",
    "sol_analytical = solve_ivp(derivatives_fnc, [t_start,10], [x_0, y_0, z_0], t_eval=t.detach())\n",
    "\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "\n",
    "ax.plot(sol_analytical.y[0], sol_analytical.y[1], sol_analytical.y[2], lw=5)\n",
    "ax.set_xlabel(\"X Axis\")\n",
    "ax.set_ylabel(\"Y Axis\")\n",
    "ax.set_zlabel(\"Z Axis\")\n",
    "ax.set_title(\"Lorenz Attractor\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
