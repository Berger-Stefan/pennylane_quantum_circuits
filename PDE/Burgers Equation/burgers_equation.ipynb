{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T21:15:20.309374Z",
     "start_time": "2024-05-12T21:15:15.069414Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from IPython import display\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "torch.manual_seed(42)\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "## QCircuit parameter\n",
    "n_wires = 8\n",
    "n_layers = 2\n",
    "\n",
    "## Optimizer Settings\n",
    "boundary_loss_scaling = 2\n",
    "\n",
    "# Domain Settings\n",
    "n_t_steps = 50\n",
    "t_prop = (0,1.0,n_t_steps)\n",
    "n_x_steps = 50\n",
    "x_prop = (-1,1,n_x_steps)\n",
    "\n",
    "# Boundary values\n",
    "#        bd1\n",
    "#       |---|\n",
    "#  bd4  |   | bd2 (not used)\n",
    "#       |---|\n",
    "#        bd3\n",
    "def bd1_fnc(input_values, function_values):\n",
    "    return 0 - function_values \n",
    "\n",
    "def bd3_fnc(input_values, function_values):\n",
    "    return 0 - function_values\n",
    "\n",
    "def bd4_fnc(input_values, function_values):\n",
    "    return function_values - (-torch.sin(torch.pi*input_values[:,1]))\n",
    "\n",
    "t = torch.linspace(t_prop[0],t_prop[1],t_prop[2], requires_grad=True, device=device)\n",
    "x = torch.linspace(x_prop[0],x_prop[1],x_prop[2], requires_grad=True, device=device)\n",
    "input_values = torch.tensor([x for x in itertools.product(t, x)], requires_grad=True)\n",
    "\n",
    "\n",
    "@qml.qnode(qml.device(\"default.qubit.torch\", wires=n_wires), interface='torch', diff_method=\"best\", max_diff=3)\n",
    "def circuit(input_values, weights):\n",
    "    # Embedding\n",
    "    for i in range(n_wires):\n",
    "        if i%2 == 0:\n",
    "            qml.RY(2*i*torch.tanh(input_values[1]),wires = i)\n",
    "        else:\n",
    "            qml.RY(2*i*torch.tanh(input_values[0]),wires = i)\n",
    "    #Variational ansatz\n",
    "    qml.BasicEntanglerLayers(weights=weights, wires=range(n_wires))\n",
    "    # Cost function\n",
    "    return qml.expval(qml.sum(*[qml.PauliZ(i) for i in range(n_wires)]))\n",
    "\n",
    "def my_model(input_values, weights, bias, scaling):\n",
    "    vcircuit = torch.vmap(circuit, in_dims=(0,None))\n",
    "    return scaling[0]*vcircuit(input_values, weights[0]) + bias[0]\n",
    "\n",
    "\n",
    "def differential_loss_fnc(input_values, weights, biases, scaling):\n",
    "    u_pred = my_model(input_values, weights, biases, scaling)\n",
    "    \n",
    "    \n",
    "    grad_outputs_1 = torch.ones_like(u_pred)\n",
    "    du = autograd.grad(u_pred, input_values, grad_outputs=grad_outputs_1, create_graph=True)[0]\n",
    "    du_dt_pred = du[:,0]\n",
    "    du_dx_pred = du[:,1]\n",
    "    \n",
    "    du_du_dx = autograd.grad(du_dx_pred, input_values, grad_outputs=grad_outputs_1, create_graph=True)[0]\n",
    "    du_dx_dx_pred = du_du_dx[:,1]\n",
    "    \n",
    "    res_pde = du_dt_pred  + u_pred * du_dx_pred - 0.01/torch.pi*du_dx_dx_pred\n",
    "    #res_pde = du_dt_pred + u_pred*du_dx_pred - 0.01/torch.pi*du_dx_dx_pred\n",
    "\n",
    "    return torch.mean(res_pde**2)\n",
    "\n",
    "def boundary_loss_fnc(input_values, weights, biases, scaling):\n",
    "\n",
    "    bd1_input_values = torch.vstack((t,torch.ones_like(x))).T\n",
    "    bd1_loss_val = bd1_fnc(bd1_input_values, my_model(bd1_input_values, weights, biases, scaling))\n",
    "\n",
    "    bd3_input_values = torch.vstack((t,-torch.ones_like(x))).T\n",
    "    bd3_loss_val = bd3_fnc(bd3_input_values, my_model(bd3_input_values, weights, biases, scaling))\n",
    "\n",
    "    bd4_input_values = torch.vstack((torch.zeros_like(t),x)).T\n",
    "    bd4_loss_val = bd4_fnc(bd4_input_values, my_model(bd4_input_values, weights, biases, scaling))\n",
    "\n",
    "    return boundary_loss_scaling * (torch.mean(bd1_loss_val**2) + torch.mean(bd3_loss_val**2) + torch.mean(bd4_loss_val**2))\n",
    "    \n",
    "def loss_fnc(input, weights, biases, scaling):\n",
    "    return boundary_loss_fnc(input, weights, biases, scaling) + differential_loss_fnc(input, weights, biases, scaling)\n",
    "\n",
    "def closure():\n",
    "    global lbfgs\n",
    "    lbfgs.zero_grad()\n",
    "    loss = loss_fnc(input_values,weights, biases, scaling)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def optimize(n_iter = 100, verbose=0, optimizer=\"adam\"):\n",
    "\n",
    "    def update(verbose):\n",
    "        if verbose == 1:\n",
    "            print(f\"Step: {i}  Loss: {loss}\")\n",
    "        elif verbose == 2:\n",
    "            fig = plt.figure()\n",
    "            fig.set_figheight(5)\n",
    "            fig.set_figwidth(20)\n",
    "\n",
    "            # Add energy plot on column 1\n",
    "            ax1 = fig.add_subplot(121)\n",
    "            ax1.grid()\n",
    "            ax1.plot(range(len(loss_history)), loss_history, \"black\", ls=\"solid\", lw=4, alpha=0.6)\n",
    "            ax1.plot(range(len(loss_diff_history)), loss_diff_history, \"g\", ls=\"dotted\",lw=2)\n",
    "            ax1.plot(range(len(loss_boundary_history)), loss_boundary_history, \"r\", ls=\"dashed\",lw=2)\n",
    "            ax1.plot(range(len(loss_analytical_history)), loss_analytical_history, \"b\", ls=\"dashdot\",lw=2)\n",
    "            ax1.legend([\"total loss\", \"diff loss\", \"boundary loss\", \"analytical loss\"], fontsize=9, loc=1)\n",
    "            ax1.set_yscale('log')\n",
    "            ax1.set_xlabel(\"Optimization step\", fontsize=13)\n",
    "            ax1.set_ylabel(\"Loss\", fontsize=13)\n",
    "            ax1.set_title(\"Loss\",fontsize=16)\n",
    "\n",
    "            ax2 = fig.add_subplot(122)\n",
    "            ax2.grid()\n",
    "            u = my_model(input_values, weights, biases, scaling).to(\"cpu\")\n",
    "            u = u.reshape(len(x), len(t)).detach().numpy()\n",
    "            contourf = ax2.contourf(x.to(\"cpu\").detach(), t.to(\"cpu\").detach(), u, levels=50, cmap=\"jet\")\n",
    "            ax2.set_xlabel(\"x\", fontsize=13)\n",
    "            ax2.set_ylabel(\"t\", fontsize=13)\n",
    "            ax2.set_title(\"Function Values\",fontsize=16)\n",
    "            cbar = fig.colorbar(contourf, ax=ax2, )\n",
    "            cbar.set_label('u', fontsize=13)  # Set label for the colorbar\n",
    "            \n",
    "            display.clear_output(wait=True)\n",
    "            plt.show()\n",
    "            print(f\"Step: {i}  Loss: {loss}\")\n",
    "    \n",
    "    for i in range(1,n_iter+1):\n",
    "        if optimizer == \"adam\":\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fnc(input_values,weights, biases, scaling)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if optimizer == \"lbfgs\":\n",
    "            lbfgs.step(closure)\n",
    "            loss = loss_fnc(input_values,weights, biases, scaling)\n",
    "\n",
    "        loss_history.append(loss.to(\"cpu\").detach())\n",
    "        loss_diff_history.append(differential_loss_fnc(input_values, weights, biases, scaling).to(\"cpu\").detach())\n",
    "        loss_boundary_history.append(boundary_loss_fnc(input_values, weights, biases, scaling).to(\"cpu\").detach())\n",
    "\n",
    "        if i%10 == 0 : update(verbose)\n",
    "        \n",
    "weights = [torch.rand((n_layers, n_wires), requires_grad=True, device=device)]        \n",
    "biases = [torch.rand(1, requires_grad=True, device=device)]\n",
    "scaling = [torch.rand(1, requires_grad=True, device=device)]\n",
    "\n",
    "parameters = weights + biases + scaling\n",
    "\n",
    "# Create optimizer\n",
    "lbfgs = torch.optim.LBFGS(parameters, line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "opt = torch.optim.Adam(parameters, lr=0.1)\n",
    "loss_history = []\n",
    "loss_diff_history = []\n",
    "loss_boundary_history = []\n",
    "loss_analytical_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-12T21:15:53.784068Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#opt.param_groups[0][\"lr\"] = 0.2\n",
    "%timeit -r1 -n1 optimize(n_iter=500, verbose=2, optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6439, dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit(torch.tensor([0,0]),weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
